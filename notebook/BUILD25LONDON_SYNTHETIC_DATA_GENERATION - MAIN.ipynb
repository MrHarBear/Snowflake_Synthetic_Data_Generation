{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "vms6z2rmzhf6y3gejepq",
   "authorId": "167081822753",
   "authorName": "HCHEN",
   "authorEmail": "harley.chen@snowflake.com",
   "sessionId": "30a6a066-a975-4347-b44c-85585b9787ee",
   "lastEditTime": 1738769876218
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc441053-2441-45a9-8586-cebc7c5db910",
   "metadata": {
    "name": "Title",
    "collapsed": false,
    "resultHeight": 532
   },
   "source": "#  **BUILD 2025 LONDON - SYNTHETIC DATA GENERATION FOR DEVELOPMENT AND TESTING OF ML MODELS**\n### Notebook - Main\n---\n### What We'll Do:\n1. **Data Ingestion**: Fetch customer and claims data from our database\n2. **Synthetic Data Generation**: Utilize both `cortex.complete` and `GENERATE_SYNTHETIC_DATA` procedure to create synthetic data\n2. **Data Transformation**: Utilize Snowpark DataFrames for data preparation and analysis\n3. **Model Training**: Train a XGB Classifier model\n4. **Model Registry**: Saving the model to Snowflake Model Registry\n\nRemember to add the necessary packages in the 'Packages' drop down at the top. For example,\n- `snowflake-snowpark-python`\n- `snowflake-ml-python`\n- `tabulate`\n- `seaborn`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1940f-fd92-4f0e-8934-64c548f61448",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Import_Packages",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Window\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import LongType\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import *\n# from snowflake.ml.registry import Registry\n\n# Import python packages\nimport streamlit as st\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n# import tabulate\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64277a92-71ae-41c2-bd63-869a5d752f27",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Setting",
    "resultHeight": 195
   },
   "outputs": [],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.ml import version\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(VERSION[0],VERSION[1],VERSION[2]))\n",
    "print('Snowflake ML version        : {}.{}.{}'.format(version.VERSION[0],version.VERSION[2],version.VERSION[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50437e40-bbf8-47e1-a976-4e785d7ecc10",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Read_data",
    "resultHeight": 847
   },
   "outputs": [],
   "source": "claim_data = session.read.table(\"BUILD_LONDON_25.DATA.CLAIM_DATA\")\ncustomer_data = session.read.table(\"BUILD_LONDON_25.DATA.CUSTOMER_DATA\")\n\nst.dataframe(claim_data.limit(50))\nst.dataframe(customer_data.limit(50))"
  },
  {
   "cell_type": "code",
   "id": "2f9bcfd2-c1e7-409f-a295-094ee371fb26",
   "metadata": {
    "language": "sql",
    "name": "Generate_fake_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- SELECT SNOWFLAKE.CORTEX.COMPLETE('mixtral-8x7b',\n-- ' Generate a structured dataset of fake but realistic insurance claims. The dataset should include diverse claim types, varying amounts, and different levels of complexity. The output should be in a tabular format with the following columns:\n\n-- Claim ID (Unique alphanumeric identifier)\n-- Policyholder Name (Realistic but randomly generated names)\n-- Policy Number (Unique identifier)\n-- Claim Type (Auto, Home, Health, Life, Disability, etc.)\n-- Claim Date (Random date within the last five years)\n-- Incident Description (Concise but detailed description of the claim)\n-- Claim Amount (Varied realistic claim amounts based on claim type)\n-- Status (Open, Closed, Under Investigation, Denied)\n-- Adjuster Name (Randomly generated name)\n-- Payout Amount (0 if denied, realistic value if approved)\n-- Fraud Flag (Yes/No, randomly assigned with logical probability)\n-- Ensure the dataset contains a variety of claim types, realistic descriptions, and varying monetary values. The data should mimic real-world patterns, including fraudulent claims, high-value claims, and small routine claims. Provide at least 100 entries.')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e5ccc3b-bbbc-4fdf-a6da-ee885b0f62e4",
   "metadata": {
    "language": "python",
    "name": "Generate_dataset_LLM",
    "collapsed": false
   },
   "outputs": [],
   "source": "# from snowflake.cortex import Complete\n\n# #llm = 'llama3.2-3b'\n# llm = 'claude-3-5-sonnet'\n# prompt = f\"\"\"\n# Given the schema of the dataframe: {str(customer_data.schema)}\n# A sample of the dataframe: {customer_data.sample(n=10).to_pandas().to_markdown()}. \n# Produce a synthetic dataset that follows the same structure as the schema and the provided sample data.\n# Provide 10 rows of these data.\n# In the output, do not include any descriptions. Only output the list of dictionary with column name as the key and synthetic value generated value as value.\"\"\"\n# llm_response = Complete(llm, prompt)\n# print(llm_response)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b762a89-f620-4516-a5cb-ec2011e626c9",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "Generate_dataset_proc",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CALL SNOWFLAKE.DATA_PRIVACY.GENERATE_SYNTHETIC_DATA({\n    'datasets':[\n        {\n          'input_table': 'BUILD_LONDON_25.DATA.claim_data',\n          'output_table': 'BUILD_LONDON_25.DATA.claim_data_synthetic',\n          'columns': {'policy_number': {'join_key': True}}\n        },\n        {\n          'input_table': 'BUILD_LONDON_25.DATA.customer_data',\n          'output_table': 'BUILD_LONDON_25.DATA.customer_data_synthetic',\n          'columns' : {'policy_number': {'join_key': True}}\n\n        }\n      ],\n      'replace_output_tables':True\n  });"
  },
  {
   "cell_type": "code",
   "id": "6ad0b83f-3921-42c8-bbad-9b1286785d67",
   "metadata": {
    "language": "sql",
    "name": "Show_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from BUILD_LONDON_25.DATA.claim_data_synthetic limit 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe928fc9-faed-4d68-b93a-6c7976cefecb",
   "metadata": {
    "language": "python",
    "name": "Prepare",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "claim_data_synthetic = session.read.table(\"BUILD_LONDON_25.DATA.CLAIM_DATA_SYNTHETIC\").to_pandas()\ncustomer_data_synthetic = session.read.table(\"BUILD_LONDON_25.DATA.CUSTOMER_DATA_SYNTHETIC\").to_pandas()\n\nsynthetic_data = pd.merge(how='inner', left=claim_data_synthetic, right=customer_data_synthetic, on='POLICY_NUMBER')\nsynthetic_data['IS_SYNTHETIC'] = 1\n\n\ndata = pd.merge(how='inner', left=claim_data.to_pandas(), right=customer_data.to_pandas(), on='POLICY_NUMBER')\ndata['IS_SYNTHETIC'] = 0\n\ntraining_data_df_pd = pd.concat([synthetic_data,data])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e66f885-db55-4b3a-8add-6fb7eefac62d",
   "metadata": {
    "language": "python",
    "name": "Check_Data",
    "collapsed": false
   },
   "outputs": [],
   "source": "training_data_df_pd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbf7f1ee-ce62-45b4-aa71-3d9a81ff4527",
   "metadata": {
    "language": "python",
    "name": "Data_cleansing",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "training_data_df = session.create_dataframe(training_data_df_pd)\n\ntraining_data_df = training_data_df.replace('?', None)\n\nmode_value_report = training_data_df.select(mode(col(\"POLICE_REPORT_AVAILABLE\"))).collect()[0][0]\nmode_value_authority = training_data_df.select(mode(col(\"AUTHORITIES_CONTACTED\"))).collect()[0][0]\ntraining_data_df = training_data_df.with_column(\"POLICE_REPORT_AVAILABLE\", \n    when(col(\"POLICE_REPORT_AVAILABLE\").is_null(), mode_value_report)\n    .otherwise(col(\"POLICE_REPORT_AVAILABLE\")))\n\ntraining_data_df = training_data_df.with_column(\"AUTHORITIES_CONTACTED\", \n    when(col(\"AUTHORITIES_CONTACTED\")=='0', mode_value_authority)\n    .otherwise(col(\"AUTHORITIES_CONTACTED\")))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c8fe66e-74ad-4d76-b403-971f1793b6f6",
   "metadata": {
    "language": "python",
    "name": "Distribution_comparison",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import seaborn as sns\npandas_df = training_data_df.to_pandas()\nsns.displot(pandas_df, x=\"CLAIM_AMOUNT\", hue = \"IS_SYNTHETIC\", element=\"step\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Drop_Unneeded_Fields",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Due to the high correlation between Age and policy_length_month, let's drop age.\n# Let's all drop the date fields \ntraining_data_df = training_data_df.drop([\"AGE\", \"INCIDENT_DATE\", \"POLICY_START_DATE\"])"
  },
  {
   "cell_type": "code",
   "id": "0f07bfa9-194c-4704-95ff-759a60b689f7",
   "metadata": {
    "language": "python",
    "name": "Split_data",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n\ntraining_data_df = training_data_df.with_column(\"FRAUD_REPORTED\", col(\"FRAUD_REPORTED\").astype(LongType()))\ntrain_data, test_data = training_data_df.random_split(weights = [0.8, 0.2], seed = 43)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88ab04-747c-4122-bfb9-4f196cf0e4a6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Pipeline_Generation",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Define the categories with their specific order\ncategories = {\n    \"INSURED_EDUCATION_LEVEL\": np.array([\"High School\", \"Associate\", \"College\", \"Masters\", \"JD\", \"MD\", \"PhD\"]),\n    \"INCIDENT_SEVERITY\": np.array([\"Trivial Damage\", \"Minor Damage\", \"Major Damage\", \"Total Loss\"])\n}\n# Create the OrdinalEncoder with specified categories\nOrdinalEncoding = OrdinalEncoder(\n    input_cols=[\"INSURED_EDUCATION_LEVEL\", \"INCIDENT_SEVERITY\"],\n    output_cols=[\"INSURED_EDUCATION_LEVEL_OE\", \"INCIDENT_SEVERITY_OE\"],\n    categories=categories,\n    handle_unknown=\"use_encoded_value\",\n    unknown_value=-1,\n    drop_input_cols=True\n)\n\n# Define the columns to encode\ncolumns_to_encode = [\n    \"INSURED_SEX\",\n    \"INSURED_OCCUPATION\",\n    \"INCIDENT_TYPE\",\n    \"AUTHORITIES_CONTACTED\",\n    \"POLICE_REPORT_AVAILABLE\"\n]\n# Create a OneHotEncoder instance\nOneHotEncoding = OneHotEncoder(\n    input_cols=columns_to_encode,\n    output_cols=[f\"{col}_encoded\" for col in columns_to_encode],\n    drop_input_cols=True,  # Keep original columns\n    handle_unknown='ignore'  # Ignore any unknown categories during transform\n)\n\n# Define the columns to scale\ncolumns_to_scale = [\n    'POLICY_LENGTH_MONTH',\n    'POLICY_DEDUCTABLE',\n    'POLICY_ANNUAL_PREMIUM',\n    'CLAIM_AMOUNT'\n]\n# Create the StandardScaler\nStandardScaling = StandardScaler(\n    input_cols=columns_to_scale,\n    output_cols=[f\"{col}_SCALED\" for col in columns_to_scale],\n    with_mean=True,\n    with_std=True,\n    drop_input_cols=True  # Keep original columns\n)\n\n# Determine the label column name\n# feature_columns = train_data.columns.remove('FRAUD_REPORTED_LONG')\nlabel_column = ['FRAUD_REPORTED']\noutput_column = ['PREDICTED_FRAUD']\n\n\n# # Initially, we can run this under the XGB Classifier model. However, you will notice that\n# # the model overfits on the training data and performs poorly on the test dataset\n# xgbmodel = XGBClassifier(\n#     random_state=1, \n#     #input_cols=feature_columns,    #here we are passing all columns so we have commented out. If you have specific columns set as features, you should specify them here\n#     label_cols=label_column,\n#     output_cols=output_column\n#     )\n\n\nxgb_grid_search = GridSearchCV(\n    estimator=XGBClassifier(),\n    param_grid={\n        \"n_estimators\":[10, 20],\n        \"max_depth\": [4],\n        \"learning_rate\":[0.1],\n    },\n    n_jobs = -1,\n    #input_cols=feature_columns,    #here we are passing all columns so we have commented out. \n                                    #If you have specific columns set as features, you should specify them here\n    label_cols=label_column,\n    output_cols=output_column,\n)\n\n\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"Ordinal_encoding\",OrdinalEncoding),\n        (\"OneHotEncoding\",OneHotEncoding),\n        (\"standardscaler\",StandardScaling),\n        (\"CV_XGBClassifier\", xgb_grid_search)\n    ]\n)"
  },
  {
   "cell_type": "code",
   "id": "21ef3b6c-7aaa-4fec-85e6-ecfe2bfc9a8b",
   "metadata": {
    "language": "python",
    "name": "Training_function",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "def train_model(train_data,test_data):\n    \n    xgb_gs_fitted_training = model_pipeline.fit(train_data)\n    xgb_gs_train = xgb_gs_fitted_training.predict(train_data)\n    xgb_gs_predictions = xgb_gs_fitted_training.predict(test_data)\n\n    # Let's start with the basic metric, Accuracy, which the number of correct predictions made divided by the total number of predictions made,\n    ACCURACY = accuracy_score(df=xgb_gs_predictions, y_true_col_names=label_column, y_pred_col_names=output_column)\n    \n    # RPC AUC is slightly perferred IMO. Anything above 50% or .5 is better than random guessing\n    AUC = roc_auc_score(df=xgb_gs_predictions, y_true_col_names=label_column, y_score_col_names=output_column)\n\n    return ACCURACY, AUC",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ffcdd7a-e710-47f3-a432-29cd2af69a11",
   "metadata": {
    "language": "python",
    "name": "WH_Up",
    "collapsed": false
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = LARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8eb22464-898f-4670-8891-22f5278a0374",
   "metadata": {
    "language": "python",
    "name": "Train_first_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "model_all_data = train_model(train_data,test_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5823eebd-1d28-44ac-aa41-f0b431f21877",
   "metadata": {
    "language": "python",
    "name": "Train_second_model",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "train_data_real = train_data.filter(col('IS_SYNTHETIC')==0)\nmodel_real_data = train_model(train_data_real,test_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4c91677-b997-4968-81eb-76610109a697",
   "metadata": {
    "language": "python",
    "name": "Performances_comparison",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "print('Performance with real data only')\nprint(f\"AUC: {model_real_data[1]:.4f}\")\n\nprint('-'*50)\n\nprint('Performance with real data AND synethtic data')\nprint(f\"AUC: {model_all_data[1]:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8938acc-107c-4536-81d0-3b1e25701628",
   "metadata": {
    "language": "python",
    "name": "WH_Down"
   },
   "outputs": [],
   "source": "wh = str(session.get_current_warehouse()).strip('\"')\nprint(f\"Current warehouse: {wh}\")\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())\n\nsession.sql(f\"alter warehouse {session.get_current_warehouse()} set WAREHOUSE_SIZE = XSMALL WAIT_FOR_COMPLETION = TRUE\").collect()\n\nprint(session.sql(f\"SHOW WAREHOUSES LIKE '{wh}';\").collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa232d63-7925-4334-aa8a-5c2c0669d095",
   "metadata": {
    "language": "python",
    "name": "Helper_Model_Stat",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 396
   },
   "outputs": [],
   "source": "print(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\",model_pipeline.to_sklearn().named_steps['CV_XGBClassifier'].best_params_)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9dfa318-66c6-44d2-974c-e445a3375e09",
   "metadata": {
    "language": "python",
    "name": "import_reg",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Let's now register the CV Classfier model into the model_registry\nReg = Registry(\n    session=session,\n    database_name=session.get_current_database(),\n    schema_name='data',\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4c357-3aa0-4792-a17d-4c4f7b140d8c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "model_reg",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "model_name = 'XGB_GS_FRAUD_MODEL'\nmodel_version = get_next_version(Reg, model_name)\n\nmv = Reg.log_model(xgb_gs_fitted_training,\n    model_name=model_name,\n    version_name=model_version,\n    conda_dependencies=[\"snowflake-ml-python\"],\n    comment=\"Model trained using GridsearchCV in Snowpark to predict fraud claims\",\n    #metrics={\"Acc\": ACCURACY, \"AUC\": AUC}, # We can save our model metrics here\n    options= {\"relax_version\": False}\n)\n\nm = Reg.get_model(model_name)\nm.default = model_version"
  }
 ]
}